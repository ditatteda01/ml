{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13e81e5",
   "metadata": {},
   "source": [
    "# Quick Start : RAG(FAISS) + LLM(Gemini)\n",
    "We use embedding models from HuggingFace for local embedding implemetation for a quick start customized test.\n",
    "\n",
    "## Archtecture\n",
    "```txt\n",
    "┌──────────┐    ┌─────────────┐    ┌─────────────────┐    ┌───────┐\n",
    "│  Corpus  │───▶│  Embedding  │───▶│      FAISS      │───▶│  LLM  │\n",
    "└──────────┘    └──────▲──────┘    │ ┌─────────────┐ │    └───────┘\n",
    "┌──────────┐           │           │ │  Vectorize  │ │\n",
    "│  Query   │───────────┘           │ └─────────────┘ │\n",
    "└──────────┘                       │        │        │\n",
    "                                   │ ┌─────────────┐ │\n",
    "                                   │ │    Search   │ │\n",
    "                                   │ └─────────────┘ │\n",
    "                                   └─────────────────┘\n",
    "```\n",
    "\n",
    "## Model\n",
    "- **Embedding**: `BAAI/bge-m3`\n",
    "- **Search Engine**: `FAISS`\n",
    "- **LLM**: `genai`\n",
    "\n",
    "## API Key\n",
    "You may get the Gemini API Key from [here](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "\n",
    "## Packages\n",
    "```bash\n",
    "conda create -n ml python=3.11\n",
    "conda activate ml\n",
    "conda install -c conda-forge faiss numpy\n",
    "pip install faiss-cpu   # for simple quick start\n",
    "pip install langchain-huggingface\n",
    "pip install sentence-transformers\n",
    "```\n",
    "Or you may install packages through `pip` only in your `venv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bed90b",
   "metadata": {},
   "source": [
    "## Function\n",
    "- Load embedding model\n",
    "- Add database to **FAISS** index\n",
    "- Ask LLM with compound information(self-defined prompt + RAG results + user query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b20b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def load_embedding_model(model_name):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        encode_kwargs={'normalize_embeddings': True} # Normalize embeddings for better similarity scores\n",
    "        # Optional: Specify device, e.g., model_kwargs={'device': 'cuda'} or {'device': 'cpu'}\n",
    "    )\n",
    "\n",
    "def add_to_faiss_index(embeddings):\n",
    "    # cast embeddings' type as numpy array\n",
    "    embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "    print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "    print(f\"Embeddings datatype: {embeddings_np.dtype}\")\n",
    "    \n",
    "    # create faiss index object\n",
    "    index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "    print(f\"FAISS index is trained: {index.is_trained}\")\n",
    "    print(f\"Number of vectors currently in index: {index.ntotal}\")\n",
    "\n",
    "    # add embeddings to index\n",
    "    index.add(embeddings_np)\n",
    "    print(f\"Number of vectors successfully added to index: {index.ntotal}\")\n",
    "\n",
    "    return index\n",
    "\n",
    "def query_search(index, query_embeddings, text_array, k=1):\n",
    "    # Ensure query_embeddings is a 2D numpy array of float32\n",
    "    # If you embedded multiple queries, query_embeddings might already be 2D\n",
    "    # If it's a single embedding (list or 1D array), reshape it\n",
    "    query_embeddings_np = np.array(query_embeddings).astype('float32')\n",
    "    if query_embeddings_np.ndim == 1:\n",
    "       query_embeddings_np = np.array([query_embeddings_np]) # Reshape to (1, dimension)\n",
    "    elif query_embeddings_np.shape[0] > 1:\n",
    "        print(\"Warning: query_search designed for single query embedding. Using only the first.\")\n",
    "        query_embeddings_np = query_embeddings_np[0, :] # Take only the first if multiple passed\n",
    "\n",
    "    # search response in index\n",
    "    distances, indices = index.search(query_embeddings_np, k)\n",
    "\n",
    "    # get results with valid index\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if 0 <= idx < len(text_array):\n",
    "            results.append((float(dist), text_array[idx]))\n",
    "\n",
    "    return results\n",
    "\n",
    "def ask_llm_with_rag(client, original_query, rag_results):\n",
    "    \"\"\"\n",
    "    Sends the original query and RAG results to Large Language Model for an answer.\n",
    "\n",
    "    Args:\n",
    "        client: Initialized LLM client.\n",
    "        generation_config: Configures that pass into LLM for generation.\n",
    "        original_query: The user's original question (string).\n",
    "        rag_results: List of tuples [(distance, text_snippet)] from query_search.\n",
    "\n",
    "    Returns:\n",
    "        The content of the assistant's reply (string), or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # 1. Prepare Context\n",
    "    context_snippets = [snippet for distance, snippet in rag_results]\n",
    "    if not context_snippets:\n",
    "        print(\"Warning: No relevant context found from RAG.\")\n",
    "        # Decide how to handle this: maybe ask without context or return specific message\n",
    "        # For now, let's proceed without specific context in the prompt if none found\n",
    "        formatted_context = \"No specific context was retrieved.\"\n",
    "    else:\n",
    "        # Combine snippets into a single block. For clearly delineate the context (e.g., using headings like \"Context:\", separators like ---, or explicit instructions).\n",
    "        formatted_context = \"\\n\\n---\\n\\n\".join(context_snippets) # Use --- as separator\n",
    "\n",
    "    # 2. Construct Prompt Message for User Role\n",
    "    # This template explicitly tells the model to use the provided context.\n",
    "    prompt = f\"\"\"\n",
    "                Strictly follow these instructions:\n",
    "                1.  Answer the \"User's Query\" based on the provided \"Context\".\n",
    "                2.  If the \"Context\" is empty or does not contain the answer, you must state: \"The provided context does not contain the necessary information.\" After that, you may make a reasonable inference to answer the query based on your general knowledge.\n",
    "                3.  Respond in the same language as the User's Query.\n",
    "                \n",
    "                ---\n",
    "\n",
    "                Context:\n",
    "                {formatted_context}\n",
    "\n",
    "                ---\n",
    "\n",
    "                User's Query:\n",
    "                {original_query}\n",
    "            \"\"\"\n",
    "\n",
    "    print(\"=============Input Message=============\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    print(\"=======================================\")\n",
    "\n",
    "    # 3. Call the Gemini API\n",
    "    try:\n",
    "        print(f\"\\nSending query and context to LLM...\")\n",
    "        response = client.generate_content(\n",
    "            contents=prompt\n",
    "        )\n",
    "        # 4. Extract the response content\n",
    "        assistant_reply = response.text\n",
    "        return assistant_reply\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling the LLM API: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bea9f",
   "metadata": {},
   "source": [
    "## Database & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fa4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "CORPUS = [\n",
    "    \"台北101是台灣著名的地標建築。\",\n",
    "    \"今天天氣晴朗，適合去郊外踏青。\",\n",
    "    \"人工智慧正在改變我們的生活方式。\",\n",
    "    \"這家餐廳的牛肉麵非常有名。\",\n",
    "    \"CI/CD現在似乎很流行。\",\n",
    "    \"我昨天看了一部非常感人的電影。\",\n",
    "    \"使用再生能源有助於減少碳排放。\",\n",
    "    \"他從小就對音樂有濃厚的興趣。\",\n",
    "    \"請問附近有沒有便利商店？\",\n",
    "    \"這本書深入探討了量子物理的基本概念。\",\n",
    "    \"她正在準備考研究所，希望能進入國立大學。\",\n",
    "    \"日本的櫻花季節吸引了許多觀光客。\",\n",
    "    \"我每天早上都會喝一杯黑咖啡。\",\n",
    "    \"手機沒電了，你可以借我充電器嗎？\",\n",
    "    \"這次的颱風造成了不少災情。\",\n",
    "    \"他正在學習如何開發網頁。\",\n",
    "    \"狗是人類最忠實的朋友之一。\",\n",
    "    \"這張畫是畢卡索晚年創作的作品。\",\n",
    "    \"我們正在討論公司的年度預算計畫。\",\n",
    "    \"最近股市波動很大，投資人需要多加注意。\",\n",
    "    \"她夢想成為一名太空人，探索宇宙的奧秘。\",\n",
    "    \"在知名程式裡出現的狗頭人喜歡蠟燭。\"\n",
    "]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "LLM_NAME = \"gemini-2.0-flash\"\n",
    "LLM_TEMP = 1.5  # 0.0-2.0\n",
    "LLM_INSTRUCT = \"You are a universal translator and multilingual assistant. Your primary rule is to mirror the user's language. If the user writes in Japanese, you must respond in Japanese. If they write in German, you must respond in German. If the language is ambiguous, default to English.\"\n",
    "\n",
    "# number of top-k search results to provide to LLM\n",
    "TOP_K = 5\n",
    "\n",
    "# user query to send to LLM\n",
    "QUERY = \"哪一段提到了程式設計？\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8866b7",
   "metadata": {},
   "source": [
    "## Main\n",
    "Let's DO THIS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb683de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding Model...\n",
      "Embedding Model: BAAI/bge-m3\n",
      "Generated 22 embeddings.\n",
      "Embedding dimension: 1024\n",
      "Embeddings shape: (22, 1024)\n",
      "Embeddings datatype: float32\n",
      "FAISS index is trained: True\n",
      "Number of vectors currently in index: 0\n",
      "Number of vectors successfully added to index: 22\n",
      "LLM: gemini-2.0-flash has been used.\n",
      "Query: 哪一段提到了程式設計？\n",
      "Top 5 distances and results: \n",
      "0.9522208571434021 | 我們正在討論公司的年度預算計畫。\n",
      "0.9524773955345154 | 他正在學習如何開發網頁。\n",
      "1.0860790014266968 | 在知名程式裡出現的狗頭人喜歡蠟燭。\n",
      "1.1489602327346802 | 請問附近有沒有便利商店？\n",
      "1.204961895942688 | 她正在準備考研究所，希望能進入國立大學。\n",
      "=============Input Message=============\n",
      "prompt: \n",
      "                Strictly follow these instructions:\n",
      "                1.  Answer the \"User's Query\" based on the provided \"Context\".\n",
      "                2.  If the \"Context\" is empty or does not contain the answer, you must state: \"The provided context does not contain the necessary information.\" After that, you may make a reasonable inference to answer the query based on your general knowledge.\n",
      "                3.  Respond in the same language as the User's Query.\n",
      "\n",
      "                ---\n",
      "\n",
      "                Context:\n",
      "                我們正在討論公司的年度預算計畫。\n",
      "\n",
      "---\n",
      "\n",
      "他正在學習如何開發網頁。\n",
      "\n",
      "---\n",
      "\n",
      "在知名程式裡出現的狗頭人喜歡蠟燭。\n",
      "\n",
      "---\n",
      "\n",
      "請問附近有沒有便利商店？\n",
      "\n",
      "---\n",
      "\n",
      "她正在準備考研究所，希望能進入國立大學。\n",
      "\n",
      "                ---\n",
      "\n",
      "                User's Query:\n",
      "                哪一段提到了程式設計？\n",
      "            \n",
      "=======================================\n",
      "\n",
      "Sending query and context to LLM...\n",
      "\n",
      "LLM Response:\n",
      "他正在學習如何開發網頁。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load OpenAI API Key\n",
    "    GEMINI_API_KEY = None\n",
    "    with open('token', 'r') as reader:\n",
    "        GEMINI_API_KEY = reader.readline().strip()\n",
    "    if not GEMINI_API_KEY:\n",
    "        print('Load Gemini API Key Failed. Process End.')\n",
    "        return\n",
    "\n",
    "    # Set up the local Hugging Face embedding model\n",
    "    # This will download the model files the first time you run it if not cached\n",
    "    print(\"Loading Embedding Model...\")\n",
    "    embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)\n",
    "    print(f\"Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "\n",
    "    # Get embeddings using the HuggingFaceEmbeddings object's method\n",
    "    # This runs the model LOCALLY\n",
    "    embeddings = embedding_model.embed_documents(CORPUS)\n",
    "\n",
    "    # embeddings will be a list of lists (or numpy array depending on version/config)\n",
    "    # Each inner list is the embedding vector for the corresponding text in text_array\n",
    "    print(f\"Generated {len(embeddings)} embeddings.\")\n",
    "    if embeddings:\n",
    "        print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "    # Add embedding to FAISS index\n",
    "    faiss_index = add_to_faiss_index(embeddings)\n",
    "\n",
    "    # Create Client to Gemini\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    llm_config=genai.types.GenerationConfig(\n",
    "        temperature=LLM_TEMP,\n",
    "        # max_output_tokens=300,\n",
    "        # top_p=0.8,\n",
    "        # stop_sequences=['Thank you']\n",
    "    )\n",
    "    client = genai.GenerativeModel(\n",
    "        model_name=LLM_NAME,\n",
    "        system_instruction=LLM_INSTRUCT,\n",
    "        generation_config=llm_config\n",
    "    )\n",
    "\n",
    "    print(f\"LLM: {LLM_NAME} has been used.\")\n",
    "\n",
    "    # Prepare message to send to LLM\n",
    "    # First, embed the query to search from FAISS index, and select top-k results\n",
    "    query_embedding = embedding_model.embed_query(QUERY)\n",
    "    print(f\"Query: {QUERY}\")\n",
    "\n",
    "\n",
    "    retrieved_results = query_search(faiss_index, query_embedding, CORPUS, TOP_K)\n",
    "    print(f\"Top {TOP_K} distances and results: \")\n",
    "    for dist, rslt in retrieved_results:\n",
    "        print(f\"{dist} | {rslt}\")\n",
    "\n",
    "    # Ask LLM with RAG context\n",
    "    if retrieved_results: # Proceed only if context was found\n",
    "        final_answer = ask_llm_with_rag(client, QUERY, retrieved_results)\n",
    "\n",
    "        if final_answer:\n",
    "            print(\"\\nLLM Response:\")\n",
    "            print(final_answer)\n",
    "        else:\n",
    "            print(\"\\nFailed to get response from LLM.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping LLM call as no relevant context was retrieved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
